# -*- coding: utf-8 -*-
"""Titanic.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aaicWi27KIIZD1w9Zbitt-1bzGTrNxXp
"""

import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import VotingClassifier
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.colab import drive

drive.mount('/content/drive/')
auth.authenticate_user()

gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

pd.options.mode.chained_assignment = None
# %matplotlib inline
warnings.filterwarnings('ignore')

"""**1.  Load the Data**"""

train_data = drive.CreateFile({'id': '1TNpBMpZVCbvF6hgP-Gvu5Iybu1hiKJkS'})
train_data.GetContentFile('train.csv')
test_data = drive.CreateFile({'id': '1d2_B-6SFGKtBwAeV3THr0459ClHfsK5C'})
test_data.GetContentFile('test.csv')

train_data = pd.read_csv("train.csv", index_col="PassengerId")
test_data = pd.read_csv("test.csv", index_col="PassengerId")

"""1.   Let's see some info about train_data
2.   Let's see some info about test_data
"""

print(train_data.info())
print(train_data.isna().sum())  # amount of missied values for each column

print(test_data.info())
print(test_data.isna().sum())

"""**2. Visualising the Data**


Let's see how different variables are associated with survival variable

**1)Pclass variable**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%flake8
# print(train_data["Pclass"].unique())
# train_data[['Pclass', 'Survived']].groupby(
#     ['Pclass'], as_index=False).mean().sort_values(
#         by='Survived', ascending=False)

"""There are three classes (1, 2 and 3) representing first, second or third class tickets on the boat.

As we can see with the higher Pclass passengers having a higher survival rate. 

So Pclass variable can make an impact on the final result and it makes sense to include it in the model.

**2)Name variable**
"""

print(train_data["Name"])

print(train_data.Name[1].split())

"""As we can see by splitting name we got title, surname and name of the person.

So I can suppose that the whole families either survived or died together. Maybe i'm wrong, but never mind:)

So i decided to match up surnames to group families together.

Also i wanna get titles of names in terms of there are some common ones such as Mr and Miss and some rare/unique ones such as Rev (reverend). 

I can assume that the passengers with their own titles were sagnificantly important so they might have been more likely to survive.

In order to get family name (surname) we can split name of passenger by comma and get the first entry of array.
"""

print(train_data.Name[1].split(",")[0])

"""In order to get title we can split name of passenger by comma and get the second entry of array.After split it by dot and get the first entry of array."""

print(train_data.Name[1].split(",")[1].split(".")[0])

train_data = train_data.assign(fname=train_data.Name.str.split(",").str[0])
train_data["title"] = pd.Series([i.split(",")[1].split(".")[0].strip()
                                for i in train_data.Name],
                                index=train_data.index)
test_data = test_data.assign(fname=test_data.Name.str.split(",").str[0])
test_data["title"] = pd.Series([i.split(",")[1].split(".")[0].strip()
                               for i in test_data.Name],
                               index=test_data.index)
train_data.drop("Name", axis=1, inplace=True)
test_data.drop("Name", axis=1, inplace=True)

"""Let's see the amount of unique fnames and titles."""

print(test_data.fname.nunique())
print(test_data.title.nunique())

"""Visualization for the number of values for each title."""

ts = sns.countplot(x="title", data=train_data)
ts = plt.setp(ts.get_xticklabels(), rotation=90)
print(train_data["title"].unique())
print(test_data["title"].unique())
other_titles = [title
                for title in train_data["title"]
                if title not in ["Mr", "Miss", "Master",
                                 "Mme", "Mlle", "Mrs", "Ms"]]
# these titles are more important than other common ones
other_titles.append("Dona")
print(set(other_titles))  # the unique values of other titles

"""There are a lot of uniques so I want to group them.

I will use the pandas dataframe replace and map functions for this.
"""

common = {"Mr": 0, "Miss": 1, "Ms": 1, "Mme": 1, "Mlle": 1,
                              "Mrs": 1, "Master": 2, "Other": 3}
train_data["title"] = train_data['title'].replace(other_titles, 'Other')
train_data["title"] = train_data["title"].map(common)
test_data["title"] = test_data['title'].replace(other_titles, 'Other')
test_data["title"] = test_data["title"].map(common)

print(train_data.title)
print(test_data.title.isna().sum())
print(train_data.title.nunique())

"""fname and title variables are not numerical so it makes sense to apply OneHotEncoder. 

We can not use map in this case in terms of our model will interpret these 
columns as quantitative variables and as the result will be inccorrect output.

We just wanna show to model the belonging of passenger to the specific family and title.
"""

oh = OneHotEncoder(handle_unknown="ignore", sparse=False)
train_data = train_data.join(pd.DataFrame(oh.fit_transform(
    train_data[["fname", "title"]]), index=train_data.index))
test_data = test_data.join(pd.DataFrame(oh.transform(
    test_data[["fname", "title"]]), index=test_data.index))
train_data.drop("fname", axis=1, inplace=True)
test_data.drop("fname", axis=1, inplace=True)

"""**3)Sex variable**"""

print(train_data["Sex"].unique())
train_data[['Sex', 'Survived']].groupby(['Sex'],
                                        as_index=False).mean().sort_values(
                                            by='Survived', ascending=False)

"""As we can see females have a much higher survival rate. 

It makes sense to include Sex in the model.

Across all males and females, females have a much higher survival rate. 

But what if wealthy males have a higher survival than poor females? It might make sense to segment this out explicity.

Let's create a new feature sex_class that represents all above.
"""

interactions = train_data.assign(
    sex_class=train_data['Sex'] + "_" + train_data['Pclass'].astype("str"))
interactions[['sex_class', 'Survived']].groupby(
    ['sex_class'], as_index=False).mean().sort_values(
        by='Survived', ascending=False)

train_data = train_data.assign(
    sex_class=train_data['Sex'] + "_" + train_data['Pclass'].astype("str"))
test_data = test_data.assign(
    sex_class=test_data['Sex'] + "_" + test_data['Pclass'].astype("str"))
print(train_data)

"""Pclass variable is encoded numerically, but it is ordinal and it will be treated the same as something like Age by most of the models.

So let's encode it by using dummy variables.
"""

train_data = train_data.join(pd.get_dummies(train_data['Pclass'],
                                            prefix="Pclass"))
test_data = test_data.join(pd.get_dummies(test_data['Pclass'],
                                          prefix="Pclass"))
print(train_data)

"""Let's encode Sex variable as numeric by using the map method."""

train_data["Sex"] = train_data["Sex"].map({"female": 0, "male": 1})
test_data["Sex"] = test_data["Sex"].map({"female": 0, "male": 1})

"""The same stuff for sex_class."""

sex_class = {"female_1": 0, "female_2": 1, "female_3": 2,
             "male_1": 4, "male_2": 5, "male_3": 6}
train_data["sex_class"] = train_data["sex_class"].map(sex_class)
test_data["sex_class"] = test_data["sex_class"].map(sex_class)
print(train_data.sex_class)

"""**4)Age**

Let's look at the distribution of age and see if there is any association with survival.
"""

g = sns.FacetGrid(train_data, col='Survived')
g = g.map(sns.distplot, "Age")

print(train_data.Age.isna().sum())

"""There are some missing values that need to be dealt with. 

So we can replace the missing data with the average from similar passengers. 

For example, if we're missing the age of a 1st class passenger, who is female, who embarked from C. 

We could substitute in the age of other passengers who fit that description.
"""

# %%flake8


def find_similar_passengers(id, dataset):
    subset = dataset[(dataset.title == dataset.title[id]) & (dataset.Pclass == dataset.Pclass[id])]

    if subset["Age"].mean() == "NaN":
        subset = dataset[(dataset["sex_class"] == dataset.iloc[id]["sex_class"])]

    if subset["Age"].mean() == "NaN":
        subset = dataset[(dataset["sex"] == dataset.iloc[id]["sex"])]

    age = subset["Age"].mean()
    return age


no_ages = train_data[train_data["Age"].isna()].index
for pid in no_ages:
    train_data.Age[pid] = find_similar_passengers(pid, train_data)

no_ages_test = test_data[test_data["Age"].isna()].index
for pid2 in no_ages_test:
    test_data.Age[pid2] = find_similar_passengers(pid2, test_data)

"""The missing data is filled in.

I think that children have a much higher survival rate and the elderly have a much lower.

So we can reorganise Age column by segmenting it into groups of <5, 5-65 and >65.

"""

train_data["age_group"] = pd.cut(train_data["Age"], bins=[0, 5, 65, 100],
                                 labels=[0, 1, 2]).astype("int64")
test_data["age_group"] = pd.cut(test_data["Age"], bins=[0, 5, 65, 100],
                                labels=[0, 1, 2]).astype("int64")
print(train_data)

"""**5)SibSp and Parch variables**

SibSp: The number of siblings or spouses aboard the titanic.

Parch: The number of parents/children aboard the titanic.

Both of them have the straight relation to family size, so we can add them together.
"""

train_data[['SibSp', 'Survived']].groupby(['SibSp'],
                                          as_index=False).mean().sort_values(
                                              by='Survived', ascending=False)

train_data[['Parch', 'Survived']].groupby(['Parch'],
                                          as_index=False).mean().sort_values(
                                              by='Survived', ascending=False)

"""As we can see that smaller families tended to survive more than larger families.

"""

train_data["fsize"] = train_data["SibSp"] + train_data["Parch"] + 1
test_data["fsize"] = test_data["SibSp"] + test_data["Parch"] + 1

train_data[['fsize', 'Survived']].groupby(['fsize'],
                                          as_index=False).mean().sort_values(
                                              by='Survived', ascending=False)

"""Small families (4 or less) survived better than people who were alone or in bigger families.

**6)Ticket**
"""

print(train_data.Ticket.nunique())
print(train_data.Ticket.isna().sum())
print(train_data.Ticket.tail())

"""As we can see tickets are numbers with some prefix letters.

Let's separate them.
"""

train_data["ticket_prefix"] = pd.Series([len(i.split()) > 1 for i in
                                         train_data.Ticket],
                                        index=train_data.index)

train_data[['ticket_prefix', 'Survived']].groupby(
    ['ticket_prefix'], as_index=False).mean().sort_values(
        by='Survived', ascending=False)

"""The survival variable doesn't depand on weather ticket has prefix or not. 

I can conclude that ticket variable makes no impact on the result.

So let's get rid of it.
"""

train_data.drop("ticket_prefix", axis=1, inplace=True)
train_data.drop("Ticket", axis=1, inplace=True)
test_data.drop("Ticket", axis=1, inplace=True)

"""**7)Fare**"""

g = sns.FacetGrid(train_data, col='Survived')
g = g.map(sns.distplot, "Fare")

"""You can see that survivors had more expensive fares and a wider spread of fare prices. 

There is at least one outlier with a fare of >500 so dropping it.

The data is pretty skewed. Take a log transformation to reduce the skew and to decrease the massive range in fares.
"""

train_data["Fare"] = train_data["Fare"].map(lambda i: np.log(i) if i > 0 else 0)
test_data["Fare"] = test_data["Fare"].map(lambda i: np.log(i) if i > 0 else 0)

g = sns.FacetGrid(train_data, col='Survived')
g = g.map(sns.distplot, "Fare")

"""**8)Cabin**

In terms of a big amout of missing values i decided to drop this column.

Also i think it makes no impact on the result.
"""

train_data.drop("Cabin", axis=1, inplace=True)
test_data.drop("Cabin", axis=1, inplace=True)

"""**9)Embarked**

 Let's fill in missing values as S.

 And encode it by using dummy variable.
"""

train_data["Embarked"] = train_data["Embarked"].fillna("S")
train_data[['Embarked', 'Survived']].groupby(
    ['Embarked'], as_index=False).mean().sort_values(
        by='Survived', ascending=False)

print(train_data.Embarked.isna().sum())

train_data = train_data.join(pd.get_dummies(train_data['Embarked'],
                                            prefix="Embarked_"))
test_data = test_data.join(pd.get_dummies(test_data['Embarked'],
                                          prefix="Embarked_"))

train_data.drop("Embarked", axis=1, inplace=True)
test_data.drop("Embarked", axis=1, inplace=True)

"""**3)Data Normalization/Standartization.**

Firstly, let's split our dataframes up into the independant variables (matrix X) 

and the dependant variable (the vector y).

In terms of our independant variables have different scale we are supposed 

to normalize it by using StandartScaler(Z-normalization).
"""

ss = StandardScaler()

train_y = train_data["Survived"]
train_data.drop("Survived", axis=1, inplace=True)

scoring_method = "f1"

train_scaled = ss.fit_transform(train_data)
test_scaled = ss.transform(test_data)

print(train_data.isna().sum())
print(test_data.isna().sum())

"""I can conclude that our data is prepared for using.

**4)Modeling**

For each model i used GridSeachCV that allows me to create 

grid of possible values for the parameters and it will test 

all possible combinations, storing the best result.

So i won't search the best parameters manually, but it's brude force algorithm and it will take a while to finish.

*1)LogisticRegression*
"""

model = LogisticRegression(random_state=10, max_iter=1000)
logit_params = {
    "C": [1, 3, 10, 20, 30, 40],
    "solver": ["lbfgs", "liblinear"]
}
logit_gs = GridSearchCV(model, logit_params, scoring="f1", cv=5, n_jobs=4)

logit_gs.fit(train_data, train_y)

print(logit_gs.best_params_)
print(logit_gs.best_score_)

"""*2)RandomForest*"""

rf_model = RandomForestClassifier()

rf_params = {
    'bootstrap': [True, False],
    'max_depth': [10, None],
    'max_features': ['auto', 'sqrt'],
    'min_samples_leaf': [1, 2, 4],
    'min_samples_split': [2, 5, 10],
    'n_estimators': [5, 10, 15, 20, 25, 30]}

rf_gs = GridSearchCV(rf_model, rf_params,
                     scoring=scoring_method, cv=8, n_jobs=4)

rf_gs.fit(train_data, train_y)

print(rf_gs.best_params_)
print(rf_gs.best_score_)

"""*3)Support Vector Machine*"""

svc_model = SVC()

test_parameters = {
    "C": [1, 3, 10, 30, 100],
    "kernel": ["linear", "poly", "rbf", "sigmoid"],
}
svc_gs = GridSearchCV(svc_model, test_parameters, scoring="f1", cv=5, n_jobs=4)

svc_gs.fit(train_scaled, train_y)

print(svc_gs.best_params_)
print(svc_gs.best_score_)

"""*4)Light Gradient Boosting*"""

lgb_model = LGBMClassifier()
test_parameters = {
    "n_estimators": [int(x) for x in np.linspace(5, 30, 6)],
    "reg_alpha": [0, 0.75, 1, 1.25],
    "learning_rate": [0.5, 0.4, 0.35, 0.3, 0.25, 0.2],
    "subsample": [0.5, 0.75, 1]
}
lgb_gs = GridSearchCV(lgb_model, test_parameters,
                      scoring=scoring_method, cv=8, n_jobs=4)

lgb_gs.fit(train_data, train_y)

print(lgb_gs.best_params_)
print(lgb_gs.best_score_)

"""*5)XGBoost*"""

xgb_model = XGBClassifier()

parameters = {'nthread': [4],
              'objective': ['binary:logistic'],
              'learning_rate': [0.05],
              'max_depth': [6],
              'min_child_weight': [11],
              'silent': [1],
              'subsample': [0.8],
              'colsample_bytree': [0.7],
              'n_estimators': [5],
              'missing': [-999],
              'seed': [1337]}


xgb_gs = GridSearchCV(xgb_model, parameters,
                      scoring=scoring_method, cv=8, n_jobs=5)

xgb_gs.fit(train_data, train_y)

print(xgb_gs.best_params_)
print(xgb_gs.best_score_)

"""*6)CatBoost*"""

catBoost = CatBoostClassifier()
test_parameters = {'iterations': [500],
                   'depth': [4, 5, 6],
                   'loss_function': ['Logloss', 'CrossEntropy'],
                   'l2_leaf_reg': np.logspace(-20, -19, 3),
                   'leaf_estimation_iterations': [10],
                   'logging_level': ['Silent'],
                   'random_seed': [42]
                   }
catboost_gs = GridSearchCV(catBoost, test_parameters,
                           scoring=scoring_method, cv=5, n_jobs=4)

catboost_gs.fit(train_data, train_y)

print(catboost_gs.best_params_)
print(catboost_gs.best_score_)

"""**5)Comparing models.**

These different models are probably placing different levels of importance on different features/variables. 

I think the key to a good ensembler/voter is to have models that have different predictions.

**6)Ensembling/Voting**

Let's use a voting classifier to use above models to make an overall prediction. 

I don't include XGBoost and CatBoost by the reason of overfitting.

When i include them i get 99 percent accuracy, but kaggle gives me worse results for this ensemble.

But without it ii get better results on keggle.

Am i correct of naming it as overfitting?

Also i faced with the problem that VotingClassifier no longer willing 

to apply all of the models during fit function working. 

The result on kaggle would be better if it worked properly.
"""

ensemble_model = VotingClassifier(estimators=[
    ("logit", logit_gs.best_estimator_),
    ("rf", rf_gs.best_estimator_),
    ("svc", svc_gs.best_estimator_),
    ("lgb", lgb_gs.best_estimator_),
    # ("xgb", xgb_gs.best_estimator_),
    # ("catboost", catboost_gs.best_estimator_),
], voting="hard")

print(ensemble_model.estimators)

ensemble_model.fit(train_data, train_y)

ensemble_model.score(train_data, train_y)

preds = ensemble_model.predict(test_data)

output = pd.DataFrame({'PassengerId': test_data.index,
                       'Survived': preds})

output.to_csv('submission.csv', index=False)